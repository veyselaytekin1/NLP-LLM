{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schema of the technic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2023-12-30-23-52-11.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1- api_key .env aldim\n",
    "# 2- PDF sayfalarini birlestirip texts adinda topladi\n",
    "# 3- CharacterTextSplitter ile chunklara böldü- OpenAI text-ada002 modelini kullandi orda tokensize 1000 idi  bizde chunk size 1000 aldik  gecemeyiz bu sayiyi\n",
    "# 4- OpenAIEmbeddings() bunu cagirdik\n",
    "# 5- docsearch = FAISS.from_texts(texts, embeddings) bunun ile PDF dosysini OpenAI tanimis oldu yani PDF embeddingslere ayrildi\n",
    "# 6- chain = load_qa_chain(OpenAI(), chain_type='stuff') langchainin load_qa_chain sayesinde sorular sormaya baslayabildik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eger fazla sayida PDF dosyalarin varsa bunlarin hepsini modeline train etmen zor olur.\n",
    "# onlarin icinden gerekli yeri alip LLm onlardan anlamli bir cevap verecek\n",
    "# önce PDF icerigi mesela 10 parcaya bölünür\n",
    "# chunks sayisi token size dan kücük olmali (bu bilgi emin degilim)\n",
    "\n",
    "# sonra her bir chunk'i embeddings vectörlerine ceviriyoruz. \n",
    "# galiba bunlari modelin tokenizeri ile yapiyoruz\n",
    "# burda ChatGPT embeddingsi kullanacagiz\n",
    "# bu sekilde bir knowladge base olusturacagi. galiba bu sekilde PDF lerin konusu anlasilmis olacak model tarafindan\n",
    "\n",
    "# eger user soru sordugunda soruda OpenAI embeddings tarafindan embeddings vectörlerine cevrilecek\n",
    "# cünkü ayni embeddings tarafinan yapilmasi gerekiyorki sayilarin anlamlari ayni olsun\n",
    "# sonra semantic search bölümünde sorunun anlami bulunmaya calisiliyor\n",
    "# ve knowledge base de bir sorgu ile bulunmaya calisiliyor\n",
    "\n",
    "# ve PDF ler icinden bulunan sonuclarin bir rank degerleri oluyor\n",
    "# bu degerler sonra LLM generative AI bölümüne aktariliyor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install openai\n",
    "# ! pip install PyPDF2\n",
    "# pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import ElasticVectorSearch, Pinecone, Weaviate, FAISS\n",
    "\n",
    "# ElasticVectorSearch : bu bir database denebilir. metinleri güclü ve hizli bir sekilde aramaniza yardimci olabilir\n",
    "# Pinecone :  vectör arama icin tasarlanmistir\n",
    "# Weaviate ; metin,kod, görüntü, ses gibi dosyalari depolayabilir.bir vectör database diyebiirz. \n",
    "\n",
    "# FAISS : Facebook tarafindan gelistirilmis vectör database lerde benzerlik aramasi yapar. \n",
    "# galiba PDF lerden ortak cevabi cikarmak icin olabilir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# .env dosyasını yükle\n",
    "load_dotenv()\n",
    "\n",
    "# Ortam değişkenini al\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PyPDF2._reader.PdfReader at 0x7f987a2b7280>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = PdfReader('/Users/veyselaytekin/Desktop/byte/NLP-LLM/ChatGPT_with_Langchain_for_own_PDF_files/2023_GPT4All_Technical_Report.pdf')\n",
    "reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = ''\n",
    "for i, page in enumerate(reader.pages):\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text +=text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPT4All: Training an Assistant-style Chatbot with Large Scale Data\\nDistillation from GPT-3.5-Turbo\\nYuvanesh Anand\\nyuvanesh@nomic.aiZach Nussbaum\\nzanussbaum@gmail.com\\nBrandon Duderstadt\\nbrandon@nomic.aiBenjamin Schmidt\\nben@nomic.aiAndriy Mulyar\\nandriy@nomic.ai\\nAbstract\\nThis preliminary technical repo'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPT4All: Training an Assistant-style Chatbot with Large Scale Data\\nDistillation from GPT-3.5-Turbo\\nYuvanesh Anand\\nyuvanesh@nomic.aiZach Nussbaum\\nzanussbaum@gmail.com\\nBrandon Duderstadt\\nbrandon@nomic.aiBenjamin Schmidt\\nben@nomic.aiAndriy Mulyar\\nandriy@nomic.ai\\nAbstract\\nThis preliminary technical report describes the\\ndevelopment of GPT4All, a chatbot trained\\nover a massive curated corpus of assistant in-\\nteractions including word problems, story de-\\nscriptions, multi-turn dialogue, and code. We\\nopenly release the collected data, data cura-\\ntion procedure, training code, and final model\\nweights to promote open research and repro-\\nducibility. Additionally, we release quantized\\n4-bit versions of the model allowing virtually\\nanyone to run the model on CPU.\\n1 Data Collection and Curation\\nWe collected roughly one million prompt-\\nresponse pairs using the GPT-3.5-Turbo OpenAI\\nAPI between March 20, 2023 and March 26th,\\n2023. To do this, we first gathered a diverse sam-'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = '\\n',\n",
    "    chunk_size = 1000, # bu openai tokensize boyutunu gecmemesi gerekir. her bir 1000 karakter 1 chunks oluyor\n",
    "    chunk_overlap = 200, # bu chunks lar arasi ortak olan karakter sayisi asagida aciklamasi var\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_text(raw_text)\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We collected roughly one million prompt-\\nresponse pairs using the GPT-3.5-Turbo OpenAI\\nAPI between March 20, 2023 and March 26th,\\n2023. To do this, we first gathered a diverse sam-\\nple of questions/prompts by leveraging three pub-\\nlicly available datasets:\\n• The unified chip2 subset of LAION OIG.\\n• Coding questions with a random sub-sample\\nof Stackoverflow Questions\\n• Instruction-tuning with a sub-sample of Big-\\nscience/P3\\nWe chose to dedicate substantial attention to data\\npreparation and curation based on commentary in\\nthe Stanford Alpaca project (Taori et al., 2023).\\nUpon collection of the initial dataset of prompt-\\ngeneration pairs, we loaded data into Atlas for data\\ncuration and cleaning. With Atlas, we removed all\\nexamples where GPT-3.5-Turbo failed to respond\\nto prompts and produced malformed output. This\\nreduced our total number of examples to 806,199\\nhigh-quality prompt-generation pairs. Next, we\\ndecided to remove the entire Bigscience/P3 sub-'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1]\n",
    "# 0. chunkta son cümlesi bunun ilk cümlesi olmus. normalde bunu yapmak zorunda degiliy ama\n",
    "# videoda faydali oluyor dedi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts) # toplamda 8 chunks var yani bu file yaklasik 8000 harf(karakter) civarinda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download embeddings from OpenAI\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# embeddings\n",
    "\n",
    "# bu embeddings i yazdirinca model ile ilgili bilgiler ve chunks sayisi yaziyor. 1000 galiba bizde bundan dolayi 1000 i gecmedik\n",
    "# bunu print yapmiyorum cünkü benim api_key i de cikti olarak veriyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x7f98aab1a4c0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch = FAISS.from_texts(texts, embeddings)\n",
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(OpenAI(), chain_type='stuff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'who are the authors of this article?'\n",
    "docs = docsearch.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The model was trained with LoRA (Hu et al., 2021) on the 437,605 post-processed examples for four epochs. Detailed model hyper-parameters and training code can be found in the associated repository and model training log.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2 = 'How was the model trained?'\n",
    "docs = docsearch.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' GPT4All is different from other models because it is trained on a massive curated corpus of assistant interactions, including word problems, story descriptions, multi-turn dialogue, and code. Additionally, the collected data, data curation procedure, training code, and final model weights are openly released to promote open research and reproducibility. Quantized 4-bit versions of the model are also released, allowing virtually anyone to run the model on CPU.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_3 = 'How is different from other models?'\n",
    "docs = docsearch.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' There is no BARD mentioned in this context, so the answer is not known.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_4 = 'What is the BARD?'\n",
    "docs = docsearch.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query_4)\n",
    "\n",
    "# PDF lerde olmayan bilgileri bilemiyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7c19a79b9aeb8b1cc18eda6778f62d726c8b19540b84d23ad80114035b2e0b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
