{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain is a framework\n",
    "# Wir k√∂nnen es mit Huggingface models and OpenAI models verwenden\n",
    "# und macht die Integration zwischen unseren File, application, our api data und LLM\n",
    "# Zum Beispiel werden wir unten eine Beispiel machen. In diesen Beispiel verbinden wir unsere OpenAI LLM\n",
    "# with google search result with eine Python library called google-search-results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install openai\n",
    "# ! pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-Y################################xjJnt'\n",
    "# This key removed from my OpenAI website. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building A Language Model Application\n",
    "#### LLMS : Get predictions from a language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Rome, Italy \n",
      "2. Bologna, Italy \n",
      "3. Florence, Italy \n",
      "4. Venice, Italy \n",
      "5. Naples, Italy\n"
     ]
    }
   ],
   "source": [
    "text = 'What are  5 vacation destinations for someone who likes to eat pasta?'\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables = ['food'],\n",
    "    template= 'What are 5 vacation destinations for someone who likes to eat {food}?'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are 5 vacation destinations for someone who likes to eat ['dessert']?\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(food=['dessert']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Paris, France - renowned for its desserts such as crepes, √©clairs, macarons, and more.\n",
      "\n",
      "2. Tokyo, Japan - a great destination for all sorts of desserts from traditional Japanese sweets to modern creations.\n",
      "\n",
      "3. Lujiazui, China - known as ‚ÄúDessert Heaven‚Äù, this area is filled with bakeries and caf√©s devoted to sweet treats like ice cream, cakes, and more.\n",
      "\n",
      "4. Bali, Indonesia - home to some of the most unique and exciting desserts like pandan cake, tropical coconut desserts, and sticky rice treats.\n",
      "\n",
      "5. New York City, USA - the city that never sleeps offers some of the world‚Äôs best ice cream, cheesecakes, cupcakes, and more.\n"
     ]
    }
   ],
   "source": [
    "print(llm(prompt.format(food=['dessert'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains : Combine LLMs and promts in multi-step workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['foof'],\n",
    "    template='What are 5 vacation destinations for someone who likes to eat {food}?'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. The Caribbean - There are so many delicious fruits found in the Caribbean like mango, papaya, pineapple, guava, and coconuts.\n",
      "\n",
      "2. South Africa - South Africa has an abundance of fresh, delicious fruits such as apricots, litchi, mangoes, and more.\n",
      "\n",
      "3. Thailand - There are many exotic fruits like durian, rambutan, mangosteen, and more to try in Thailand.\n",
      "\n",
      "4. Mexico - Mexico is a great destination for fruit-lovers, with tons of tropical fruits like bananas, plantains, papayas and more.\n",
      "\n",
      "5. The Mediterranean - The Mediterranean offers plenty of delicious citrus fruits, figs, pomegranates, and olives.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run('fruit'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents : Dynamically call chains based on user input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We get with google-search-results python library the google's results than use our model this infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# serpapi is for google search\n",
    "os.environ['SERPAPI_API_KEY'] = 'dd31ad###################################9ac8c'\n",
    "\n",
    "tools  = load_tools(['serpapi', 'llm-math'], llm=llm)\n",
    "\n",
    "# google sonuclarina erismek icin kullanioyruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, tags=['zero-shot-react-description'], agent=ZeroShotAgent(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'input'], template='Answer the following questions as best you can. You have access to the following tools:\\n\\nSearch: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\\nCalculator: Useful for when you need to answer questions about math.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Search, Calculator]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}'), llm=OpenAI(client=<openai.resources.completions.Completions object at 0x7f9bb8c26f10>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7f9ba88ffb20>, temperature=0.0, openai_api_key='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', openai_proxy='')), output_parser=MRKLOutputParser(), allowed_tools=['Search', 'Calculator']), tools=[Tool(name='Search', description='A search engine. Useful for when you need to answer questions about current events. Input should be a search query.', func=<bound method SerpAPIWrapper.run of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='dd31ad86692e4ad26e6e2a99aba7aaebffe5ffdc5cf6230f44fb8f34c859ac8c', aiosession=None)>, coroutine=<bound method SerpAPIWrapper.arun of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='dd31ad86692e4ad26e6e2a99aba7aaebffe5ffdc5cf6230f44fb8f34c859ac8c', aiosession=None)>), Tool(name='Calculator', description='Useful for when you need to answer questions about math.', func=<bound method Chain.run of LLMMathChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=OpenAI(client=<openai.resources.completions.Completions object at 0x7f9bb8c26f10>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7f9ba88ffb20>, temperature=0.0, openai_api_key='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', openai_proxy='')))>, coroutine=<bound method Chain.arun of LLMMathChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=OpenAI(client=<openai.resources.completions.Completions object at 0x7f9bb8c26f10>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7f9ba88ffb20>, temperature=0.0, openai_api_key='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', openai_proxy='')))>)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = initialize_agent(tools, llm, agent='zero-shot-react-description', verbose=True)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who the leader of Japan is and then calculate the largest prime number that is smaller than their age.\n",
      "Action: Search\n",
      "Action Input: \"Current leader of Japan\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mFumio Kishida is the current prime minister of Japan, replacing Yoshihide Suga on 4 October 2021. As of 22 December 2023, there have been 64 individual prime ministers serving 101 terms of office.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to calculate the largest prime number that is smaller than the prime minister's age.\n",
      "Action: Calculator\n",
      "Action Input: 64\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 64\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: The current leader of Japan is Fumio Kishida and the largest prime number that is smaller than their age is 64.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current leader of Japan is Fumio Kishida and the largest prime number that is smaller than their age is 64.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's test it out\n",
    "agent.run('Who is the current leader of japan? What is the largest prime number that is smaller than their age?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory : Add state to chains and agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Hi there! How can I help you?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='Hi there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there\n",
      "AI:  Hi there! How can I help you?\n",
      "Human: I'm doing well. Just having a conversation with an AI\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That's great! It's always nice to have a conversation with someone. What would you like to talk about?\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I'm doing well. Just having a conversation with an AI\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there\n",
      "AI:  Hi there! How can I help you?\n",
      "Human: I'm doing well. Just having a conversation with an AI\n",
      "AI:  That's great! It's always nice to have a conversation with someone. What would you like to talk about?\n",
      "Human: What was the first thing I said to you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' You said \"Hi there\"!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What was the first thing I said to you?\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Components\n",
    "#### Schema - Nuts and Bolts of working with LLMs\n",
    "#### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with simple strings (that'll soon frow in complexity)\n",
    "my_text = 'What day comes after Freiday?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Messages\n",
    "System message : this background context that tell the AI what to do  \n",
    "\n",
    "\n",
    "Human : input comes from user  \n",
    "\n",
    "AI : Output comes from AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = \"sk-KuE8CMF##################################ZXZXSlDEjjP7\"\n",
    "# This key removed from my OpenAI website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.7, openai_api_key=openai_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You could try a fresh Caprese salad with tomatoes, mozzarella, and basil.')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [SystemMessage(content='You are a nice AI bot that helps a user figure out what to eat in one short sentence'),\n",
    "    HumanMessage(content='I like tomatoes, what should I eat?')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass more chat history responses from the AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auf Deutsch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Wenn du in Nizza bist, gibt es viele Aktivit√§ten, die du genie√üen kannst. Hier sind einige Vorschl√§ge:\\n\\n1. Erkunde die Altstadt (Vieux Nice): Schlendere durch die engen Gassen, bewundere die bunten Geb√§ude und besuche den ber√ºhmten Blumenmarkt.\\n\\n2. Besuche den Promenade des Anglais: Diese ber√ºhmte Strandpromenade erstreckt sich entlang der K√ºste von Nizza und bietet einen atemberaubenden Blick auf das Meer. Du kannst hier spazieren gehen, Rad fahren oder einfach nur die Sonne genie√üen.\\n\\n3. Entdecke die Museen: Nizza beherbergt eine Vielzahl von Museen, darunter das Mus√©e Matisse, das Mus√©e d'Art Moderne et d'Art Contemporain und das Mus√©e Marc Chagall. Hier kannst du die Werke bekannter K√ºnstler bewundern.\\n\\n4. Besuche den Markt Cours Saleya: Dieser Markt ist ber√ºhmt f√ºr seine frischen Lebensmittel, lokale Produkte und Blumen. Hier kannst du lokale Spezialit√§ten probieren und Souvenirs kaufen.\\n\\n5. Genie√üe die lokale K√ºche: Nizza ist bekannt f√ºr seine k√∂stliche K√ºche, insbesondere f√ºr Gerichte wie Salade Ni√ßoise, Socca (eine Art Kichererbsenpfannkuchen) und Ratatouille. Probiere unbedingt diese lokalen Spezialit√§ten.\\n\\n6. Mach einen Ausflug nach Monaco: Nizza liegt in der N√§he von Monaco, einem kleinen F√ºrstentum, das f√ºr seinen Glamour und seine Casinos bekannt ist. Du kannst einen Tagesausflug nach Monaco unternehmen und die Sehensw√ºrdigkeiten wie das Casino von Monte Carlo und den F√ºrstenpalast besichtigen.\\n\\nDas sind nur einige der vielen M√∂glichkeiten, wie du deine Zeit in Nizza verbringen kannst. Es gibt noch viel mehr zu entdecken und zu erleben.\")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content='Du bist eine sch√∂ne AI bot. Du hilfst die users'),\n",
    "        HumanMessage(content='I w√ºrde gern reisen?'),\n",
    "        AIMessage(content= 'Du sollst nach Nice, Frankreich fahren'),\n",
    "        HumanMessage(content='Was mache ich, wenn ich da w√§re?')\n",
    "    ]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='This is my document. It is full of text that I have gathered from other places ', metadata={'my_document_id': 234234, 'my_document_source': 'The Langchain Papers', 'my_document_create_time': 1680013019})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content='This is my document. It is full of text that I have gathered from other places ',\n",
    "metadata={\n",
    "    'my_document_id' : 234234,\n",
    "    'my_document_source' : 'The Langchain Papers',\n",
    "    'my_document_create_time' : 1680013019\n",
    "})\n",
    "# if you have more document you can filter with that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models - Interface to the AI brains\n",
    "### Language Model\n",
    "a model that does text in -> text out\n",
    "\n",
    "\n",
    "Check out how I changed tha model I was using from the default one to ada-001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSaturday.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name = 'text-ada-001', openai_api_key = openai_api_key)\n",
    "# here there are a lot of model from OpenAI https://platform.openai.com/account/limits\n",
    "\n",
    "llm('What day comes after Friday?')\n",
    "\n",
    "# bu cok cikti vermeyen bir model galiba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=1, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Why don't you just ask a pigeon for directions? They're practically the unofficial New York tour guides! üê¶\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content='You are an unhelpful AI bot that make a joke at whatever the user says. '),\n",
    "        HumanMessage(content='I would like to go to New York, how should I do this?')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# AI model gives your answer according System message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Embedding Model\n",
    "Change your text into the vector(a series of numbers that hold the semantic 'meaning' of your text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.5.2-cp39-cp39-macosx_10_9_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/veyselaytekin/opt/anaconda3/lib/python3.9/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/veyselaytekin/opt/anaconda3/lib/python3.9/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/veyselaytekin/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/veyselaytekin/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/veyselaytekin/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/veyselaytekin/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
      "Downloading tiktoken-0.5.2-cp39-cp39-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.5.2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Hi It is time for the beach'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your embedding is lenght 1536\n",
      "Here is a sample: [0.0018777001605210632, 0.006782962756549415, -0.0034220627511661163, -0.021981937214207405, -0.012550621884194459] ... \n"
     ]
    }
   ],
   "source": [
    "text_embedding = embeddings.embed_query(text)\n",
    "print(f'Your embedding is lenght {len(text_embedding)}')\n",
    "print(f'Here is a sample: {text_embedding[:5]} ... ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = 'Hi It is time for the beach'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your embedding is lenght 1536\n",
      "Here is a sample: [0.0018777001605210632, 0.006782962756549415, -0.0034220627511661163, -0.021981937214207405, -0.012550621884194459] ... \n"
     ]
    }
   ],
   "source": [
    "text_embedding = embeddings.embed_query(text_2)\n",
    "print(f'Your embedding is lenght {len(text_embedding)}')\n",
    "print(f'Here is a sample: {text_embedding[:5]} ... ')\n",
    "\n",
    "# the numbers take the values between -1 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt - Text used as instractions to your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\\nThe statement is incorrect because tomorrow is Tuesday, not Wednesday.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name = 'text-davinci-003', openai_api_key=openai_api_key)\n",
    "\n",
    "\n",
    "prompt = \"\"\"Today is Monday, tomorrow is Wednesday \n",
    "What is wrong in this statement?\"\"\"\n",
    "\n",
    "# to read easier put three double quotation marks\n",
    "\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prompt: \n",
      "T really want to travel to Rome. What should I do there?\n",
      "REspond in one short sentence\n",
      "\n",
      "--------------------------------------------------\n",
      "LLM Output: \n",
      "Explore the ancient ruins, iconic monuments, and vibrant culture of Rome.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name='text-davinci-003', openai_api_key=openai_api_key)\n",
    "\n",
    "template = \"\"\"\n",
    "T really want to travel to {location}. What should I do there?\n",
    "REspond in one short sentence\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = ['location'],\n",
    "    template = template\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location='Rome')\n",
    "\n",
    "print(f'Final Prompt: {final_prompt}')\n",
    "print('-'*50)\n",
    "print(f'LLM Output: {llm(final_prompt)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples Selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name = 'text-davinci-003', openai_api_key=openai_api_key)\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=['input', 'output'],\n",
    "    template='Example Input: {input} \\nExample Output: {output}'\n",
    ")\n",
    "\n",
    "# Examples of lacation that nouns are found\n",
    "examples = [\n",
    "    {'input' : 'pirate', 'output' : 'ship'},\n",
    "    {'input' : 'pilot', 'output' : 'plane'},\n",
    "    {'input' : 'driver', 'output' : 'car'},\n",
    "    {'input' : 'tree', 'output' : 'ground'},\n",
    "    {'input' : 'bird', 'output' : 'nest'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_community/vectorstores/faiss.py:56\u001b[0m, in \u001b[0;36mdependable_faiss_import\u001b[0;34m(no_avx2)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m         \u001b[39mimport\u001b[39;00m \u001b[39mfaiss\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# SemanticSimilarityExampleSelector will select examples that are similiar to your input \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m example_selector \u001b[38;5;241m=\u001b[39m \u001b[43mSemanticSimilarityExampleSelector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_examples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# this is a list os examples avaliable to select from.\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopenai_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopenai_api_key\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mFAISS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# This is the vectorStore class we use that if we want to find the similar input . find the neigboors vectors\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_core/example_selectors/semantic_similarity.py:97\u001b[0m, in \u001b[0;36mSemanticSimilarityExampleSelector.from_examples\u001b[0;34m(cls, examples, embeddings, vectorstore_cls, k, input_keys, **vectorstore_cls_kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     string_examples \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(sorted_values(eg)) \u001b[39mfor\u001b[39;00m eg \u001b[39min\u001b[39;00m examples]\n\u001b[0;32m---> 97\u001b[0m vectorstore \u001b[39m=\u001b[39m vectorstore_cls\u001b[39m.\u001b[39;49mfrom_texts(\n\u001b[1;32m     98\u001b[0m     string_examples, embeddings, metadatas\u001b[39m=\u001b[39;49mexamples, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mvectorstore_cls_kwargs\n\u001b[1;32m     99\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(vectorstore\u001b[39m=\u001b[39mvectorstore, k\u001b[39m=\u001b[39mk, input_keys\u001b[39m=\u001b[39minput_keys)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_community/vectorstores/faiss.py:915\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m    897\u001b[0m \n\u001b[1;32m    898\u001b[0m \u001b[39mThis is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[39m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    914\u001b[0m embeddings \u001b[39m=\u001b[39m embedding\u001b[39m.\u001b[39membed_documents(texts)\n\u001b[0;32m--> 915\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m__from(\n\u001b[1;32m    916\u001b[0m     texts,\n\u001b[1;32m    917\u001b[0m     embeddings,\n\u001b[1;32m    918\u001b[0m     embedding,\n\u001b[1;32m    919\u001b[0m     metadatas\u001b[39m=\u001b[39;49mmetadatas,\n\u001b[1;32m    920\u001b[0m     ids\u001b[39m=\u001b[39;49mids,\n\u001b[1;32m    921\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    922\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_community/vectorstores/faiss.py:869\u001b[0m, in \u001b[0;36mFAISS.__from\u001b[0;34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    858\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__from\u001b[39m(\n\u001b[1;32m    859\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    868\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m FAISS:\n\u001b[0;32m--> 869\u001b[0m     faiss \u001b[39m=\u001b[39m dependable_faiss_import()\n\u001b[1;32m    870\u001b[0m     \u001b[39mif\u001b[39;00m distance_strategy \u001b[39m==\u001b[39m DistanceStrategy\u001b[39m.\u001b[39mMAX_INNER_PRODUCT:\n\u001b[1;32m    871\u001b[0m         index \u001b[39m=\u001b[39m faiss\u001b[39m.\u001b[39mIndexFlatIP(\u001b[39mlen\u001b[39m(embeddings[\u001b[39m0\u001b[39m]))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_community/vectorstores/faiss.py:58\u001b[0m, in \u001b[0;36mdependable_faiss_import\u001b[0;34m(no_avx2)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[39mimport\u001b[39;00m \u001b[39mfaiss\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     59\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import faiss python package. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install it with `pip install faiss-gpu` (for CUDA supported GPU) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `pip install faiss-cpu` (depending on Python version).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m     )\n\u001b[1;32m     63\u001b[0m \u001b[39mreturn\u001b[39;00m faiss\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version)."
     ]
    }
   ],
   "source": [
    "# SemanticSimilarityExampleSelector will select examples that are similiar to your input \n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "\n",
    "    examples, # this is a list os examples avaliable to select from.\n",
    "\n",
    "    OpenAIEmbeddings(openai_api_key=openai_api_key),\n",
    "\n",
    "    FAISS, # This is the vectorStore class we use that if we want to find the similar input . find the neigboors vectors\n",
    "\n",
    "    k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7c19a79b9aeb8b1cc18eda6778f62d726c8b19540b84d23ad80114035b2e0b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
