{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain is a framework\n",
    "# Wir können es mit Huggingface models and OpenAI models verwenden\n",
    "# und macht die Integration zwischen unseren File, application, our api data und LLM\n",
    "# Zum Beispiel werden wir unten eine Beispiel machen. In diesen Beispiel verbinden wir unsere OpenAI LLM\n",
    "# with google search result with eine Python library called google-search-results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install openai\n",
    "# ! pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-Y################################xjJnt'\n",
    "# This key removed from my OpenAI website. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building A Language Model Application\n",
    "#### LLMS : Get predictions from a language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Rome, Italy \n",
      "2. Bologna, Italy \n",
      "3. Florence, Italy \n",
      "4. Venice, Italy \n",
      "5. Naples, Italy\n"
     ]
    }
   ],
   "source": [
    "text = 'What are  5 vacation destinations for someone who likes to eat pasta?'\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables = ['food'],\n",
    "    template= 'What are 5 vacation destinations for someone who likes to eat {food}?'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are 5 vacation destinations for someone who likes to eat ['dessert']?\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(food=['dessert']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Paris, France - renowned for its desserts such as crepes, éclairs, macarons, and more.\n",
      "\n",
      "2. Tokyo, Japan - a great destination for all sorts of desserts from traditional Japanese sweets to modern creations.\n",
      "\n",
      "3. Lujiazui, China - known as “Dessert Heaven”, this area is filled with bakeries and cafés devoted to sweet treats like ice cream, cakes, and more.\n",
      "\n",
      "4. Bali, Indonesia - home to some of the most unique and exciting desserts like pandan cake, tropical coconut desserts, and sticky rice treats.\n",
      "\n",
      "5. New York City, USA - the city that never sleeps offers some of the world’s best ice cream, cheesecakes, cupcakes, and more.\n"
     ]
    }
   ],
   "source": [
    "print(llm(prompt.format(food=['dessert'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains : Combine LLMs and promts in multi-step workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['foof'],\n",
    "    template='What are 5 vacation destinations for someone who likes to eat {food}?'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. The Caribbean - There are so many delicious fruits found in the Caribbean like mango, papaya, pineapple, guava, and coconuts.\n",
      "\n",
      "2. South Africa - South Africa has an abundance of fresh, delicious fruits such as apricots, litchi, mangoes, and more.\n",
      "\n",
      "3. Thailand - There are many exotic fruits like durian, rambutan, mangosteen, and more to try in Thailand.\n",
      "\n",
      "4. Mexico - Mexico is a great destination for fruit-lovers, with tons of tropical fruits like bananas, plantains, papayas and more.\n",
      "\n",
      "5. The Mediterranean - The Mediterranean offers plenty of delicious citrus fruits, figs, pomegranates, and olives.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run('fruit'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents : Dynamically call chains based on user input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We get with google-search-results python library the google's results than use our model this infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# serpapi is for google search\n",
    "os.environ['SERPAPI_API_KEY'] = 'dd31ad###################################9ac8c'\n",
    "\n",
    "tools  = load_tools(['serpapi', 'llm-math'], llm=llm)\n",
    "\n",
    "# google sonuclarina erismek icin kullanioyruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, tags=['zero-shot-react-description'], agent=ZeroShotAgent(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'input'], template='Answer the following questions as best you can. You have access to the following tools:\\n\\nSearch: A search engine. Useful for when you need to answer questions about current events. Input should be a search query.\\nCalculator: Useful for when you need to answer questions about math.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Search, Calculator]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}'), llm=OpenAI(client=<openai.resources.completions.Completions object at 0x7f9bb8c26f10>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7f9ba88ffb20>, temperature=0.0, openai_api_key='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', openai_proxy='')), output_parser=MRKLOutputParser(), allowed_tools=['Search', 'Calculator']), tools=[Tool(name='Search', description='A search engine. Useful for when you need to answer questions about current events. Input should be a search query.', func=<bound method SerpAPIWrapper.run of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='dd31ad86692e4ad26e6e2a99aba7aaebffe5ffdc5cf6230f44fb8f34c859ac8c', aiosession=None)>, coroutine=<bound method SerpAPIWrapper.arun of SerpAPIWrapper(search_engine=<class 'serpapi.google_search.GoogleSearch'>, params={'engine': 'google', 'google_domain': 'google.com', 'gl': 'us', 'hl': 'en'}, serpapi_api_key='dd31ad86692e4ad26e6e2a99aba7aaebffe5ffdc5cf6230f44fb8f34c859ac8c', aiosession=None)>), Tool(name='Calculator', description='Useful for when you need to answer questions about math.', func=<bound method Chain.run of LLMMathChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=OpenAI(client=<openai.resources.completions.Completions object at 0x7f9bb8c26f10>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7f9ba88ffb20>, temperature=0.0, openai_api_key='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', openai_proxy='')))>, coroutine=<bound method Chain.arun of LLMMathChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=OpenAI(client=<openai.resources.completions.Completions object at 0x7f9bb8c26f10>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7f9ba88ffb20>, temperature=0.0, openai_api_key='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', openai_proxy='')))>)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = initialize_agent(tools, llm, agent='zero-shot-react-description', verbose=True)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who the leader of Japan is and then calculate the largest prime number that is smaller than their age.\n",
      "Action: Search\n",
      "Action Input: \"Current leader of Japan\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mFumio Kishida is the current prime minister of Japan, replacing Yoshihide Suga on 4 October 2021. As of 22 December 2023, there have been 64 individual prime ministers serving 101 terms of office.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to calculate the largest prime number that is smaller than the prime minister's age.\n",
      "Action: Calculator\n",
      "Action Input: 64\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 64\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: The current leader of Japan is Fumio Kishida and the largest prime number that is smaller than their age is 64.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current leader of Japan is Fumio Kishida and the largest prime number that is smaller than their age is 64.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's test it out\n",
    "agent.run('Who is the current leader of japan? What is the largest prime number that is smaller than their age?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory : Add state to chains and agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Hi there! How can I help you?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input='Hi there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there\n",
      "AI:  Hi there! How can I help you?\n",
      "Human: I'm doing well. Just having a conversation with an AI\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" That's great! It's always nice to have a conversation with someone. What would you like to talk about?\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I'm doing well. Just having a conversation with an AI\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there\n",
      "AI:  Hi there! How can I help you?\n",
      "Human: I'm doing well. Just having a conversation with an AI\n",
      "AI:  That's great! It's always nice to have a conversation with someone. What would you like to talk about?\n",
      "Human: What was the first thing I said to you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' You said \"Hi there\"!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What was the first thing I said to you?\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain Components\n",
    "#### Schema - Nuts and Bolts of working with LLMs\n",
    "#### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with simple strings (that'll soon frow in complexity)\n",
    "my_text = 'What day comes after Freiday?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Messages\n",
    "System message : this background context that tell the AI what to do  \n",
    "\n",
    "\n",
    "Human : input comes from user  \n",
    "\n",
    "AI : Output comes from AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = \"sk-KuE8CMF##################################ZXZXSlDEjjP7\"\n",
    "# This key removed from my OpenAI website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.7, openai_api_key=openai_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You could try a fresh Caprese salad with tomatoes, mozzarella, and basil.')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [SystemMessage(content='You are a nice AI bot that helps a user figure out what to eat in one short sentence'),\n",
    "    HumanMessage(content='I like tomatoes, what should I eat?')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass more chat history responses from the AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auf Deutsch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Wenn du in Nizza bist, gibt es viele Aktivitäten, die du genießen kannst. Hier sind einige Vorschläge:\\n\\n1. Erkunde die Altstadt (Vieux Nice): Schlendere durch die engen Gassen, bewundere die bunten Gebäude und besuche den berühmten Blumenmarkt.\\n\\n2. Besuche den Promenade des Anglais: Diese berühmte Strandpromenade erstreckt sich entlang der Küste von Nizza und bietet einen atemberaubenden Blick auf das Meer. Du kannst hier spazieren gehen, Rad fahren oder einfach nur die Sonne genießen.\\n\\n3. Entdecke die Museen: Nizza beherbergt eine Vielzahl von Museen, darunter das Musée Matisse, das Musée d'Art Moderne et d'Art Contemporain und das Musée Marc Chagall. Hier kannst du die Werke bekannter Künstler bewundern.\\n\\n4. Besuche den Markt Cours Saleya: Dieser Markt ist berühmt für seine frischen Lebensmittel, lokale Produkte und Blumen. Hier kannst du lokale Spezialitäten probieren und Souvenirs kaufen.\\n\\n5. Genieße die lokale Küche: Nizza ist bekannt für seine köstliche Küche, insbesondere für Gerichte wie Salade Niçoise, Socca (eine Art Kichererbsenpfannkuchen) und Ratatouille. Probiere unbedingt diese lokalen Spezialitäten.\\n\\n6. Mach einen Ausflug nach Monaco: Nizza liegt in der Nähe von Monaco, einem kleinen Fürstentum, das für seinen Glamour und seine Casinos bekannt ist. Du kannst einen Tagesausflug nach Monaco unternehmen und die Sehenswürdigkeiten wie das Casino von Monte Carlo und den Fürstenpalast besichtigen.\\n\\nDas sind nur einige der vielen Möglichkeiten, wie du deine Zeit in Nizza verbringen kannst. Es gibt noch viel mehr zu entdecken und zu erleben.\")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content='Du bist eine schöne AI bot. Du hilfst die users'),\n",
    "        HumanMessage(content='I würde gern reisen?'),\n",
    "        AIMessage(content= 'Du sollst nach Nice, Frankreich fahren'),\n",
    "        HumanMessage(content='Was mache ich, wenn ich da wäre?')\n",
    "    ]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='This is my document. It is full of text that I have gathered from other places ', metadata={'my_document_id': 234234, 'my_document_source': 'The Langchain Papers', 'my_document_create_time': 1680013019})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content='This is my document. It is full of text that I have gathered from other places ',\n",
    "metadata={\n",
    "    'my_document_id' : 234234,\n",
    "    'my_document_source' : 'The Langchain Papers',\n",
    "    'my_document_create_time' : 1680013019\n",
    "})\n",
    "# if you have more document you can filter with that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models - Interface to the AI brains\n",
    "### Language Model\n",
    "a model that does text in -> text out\n",
    "\n",
    "\n",
    "Check out how I changed tha model I was using from the default one to ada-001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSaturday.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name = 'text-ada-001', openai_api_key = openai_api_key)\n",
    "# here there are a lot of model from OpenAI https://platform.openai.com/account/limits\n",
    "\n",
    "llm('What day comes after Friday?')\n",
    "\n",
    "# bu cok cikti vermeyen bir model galiba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=1, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Why don't you just ask a pigeon for directions? They're practically the unofficial New York tour guides! 🐦\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\n",
    "    [\n",
    "        SystemMessage(content='You are an unhelpful AI bot that make a joke at whatever the user says. '),\n",
    "        HumanMessage(content='I would like to go to New York, how should I do this?')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# AI model gives your answer according System message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Embedding Model\n",
    "Change your text into the vector(a series of numbers that hold the semantic 'meaning' of your text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.5.2-cp39-cp39-macosx_10_9_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/veyselaytekin/opt/anaconda3/lib/python3.9/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/veyselaytekin/opt/anaconda3/lib/python3.9/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/veyselaytekin/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/veyselaytekin/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/veyselaytekin/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/veyselaytekin/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
      "Downloading tiktoken-0.5.2-cp39-cp39-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.5.2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Hi It is time for the beach'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your embedding is lenght 1536\n",
      "Here is a sample: [0.0018777001605210632, 0.006782962756549415, -0.0034220627511661163, -0.021981937214207405, -0.012550621884194459] ... \n"
     ]
    }
   ],
   "source": [
    "text_embedding = embeddings.embed_query(text)\n",
    "print(f'Your embedding is lenght {len(text_embedding)}')\n",
    "print(f'Here is a sample: {text_embedding[:5]} ... ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = 'Hi It is time for the beach'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your embedding is lenght 1536\n",
      "Here is a sample: [0.0018777001605210632, 0.006782962756549415, -0.0034220627511661163, -0.021981937214207405, -0.012550621884194459] ... \n"
     ]
    }
   ],
   "source": [
    "text_embedding = embeddings.embed_query(text_2)\n",
    "print(f'Your embedding is lenght {len(text_embedding)}')\n",
    "print(f'Here is a sample: {text_embedding[:5]} ... ')\n",
    "\n",
    "# the numbers take the values between -1 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt - Text used as instractions to your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\\nThe statement is incorrect because tomorrow is Tuesday, not Wednesday.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name = 'text-davinci-003', openai_api_key=openai_api_key)\n",
    "\n",
    "\n",
    "prompt = \"\"\"Today is Monday, tomorrow is Wednesday \n",
    "What is wrong in this statement?\"\"\"\n",
    "\n",
    "# to read easier put three double quotation marks\n",
    "\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prompt: \n",
      "T really want to travel to Rome. What should I do there?\n",
      "REspond in one short sentence\n",
      "\n",
      "--------------------------------------------------\n",
      "LLM Output: \n",
      "Explore the ancient ruins, iconic monuments, and vibrant culture of Rome.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model_name='text-davinci-003', openai_api_key=openai_api_key)\n",
    "\n",
    "template = \"\"\"\n",
    "T really want to travel to {location}. What should I do there?\n",
    "REspond in one short sentence\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = ['location'],\n",
    "    template = template\n",
    ")\n",
    "\n",
    "final_prompt = prompt.format(location='Rome')\n",
    "\n",
    "print(f'Final Prompt: {final_prompt}')\n",
    "print('-'*50)\n",
    "print(f'LLM Output: {llm(final_prompt)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples Selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name = 'text-davinci-003', openai_api_key=openai_api_key)\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=['input', 'output'],\n",
    "    template='Example Input: {input} \\nExample Output: {output}'\n",
    ")\n",
    "\n",
    "# Examples of lacation that nouns are found\n",
    "examples = [\n",
    "    {'input' : 'pirate', 'output' : 'ship'},\n",
    "    {'input' : 'pilot', 'output' : 'plane'},\n",
    "    {'input' : 'driver', 'output' : 'car'},\n",
    "    {'input' : 'tree', 'output' : 'ground'},\n",
    "    {'input' : 'bird', 'output' : 'nest'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_community/vectorstores/faiss.py:56\u001b[0m, in \u001b[0;36mdependable_faiss_import\u001b[0;34m(no_avx2)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m         \u001b[39mimport\u001b[39;00m \u001b[39mfaiss\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# SemanticSimilarityExampleSelector will select examples that are similiar to your input \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m example_selector \u001b[38;5;241m=\u001b[39m \u001b[43mSemanticSimilarityExampleSelector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_examples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# this is a list os examples avaliable to select from.\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopenai_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopenai_api_key\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mFAISS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# This is the vectorStore class we use that if we want to find the similar input . find the neigboors vectors\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_core/example_selectors/semantic_similarity.py:97\u001b[0m, in \u001b[0;36mSemanticSimilarityExampleSelector.from_examples\u001b[0;34m(cls, examples, embeddings, vectorstore_cls, k, input_keys, **vectorstore_cls_kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     string_examples \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(sorted_values(eg)) \u001b[39mfor\u001b[39;00m eg \u001b[39min\u001b[39;00m examples]\n\u001b[0;32m---> 97\u001b[0m vectorstore \u001b[39m=\u001b[39m vectorstore_cls\u001b[39m.\u001b[39;49mfrom_texts(\n\u001b[1;32m     98\u001b[0m     string_examples, embeddings, metadatas\u001b[39m=\u001b[39;49mexamples, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mvectorstore_cls_kwargs\n\u001b[1;32m     99\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(vectorstore\u001b[39m=\u001b[39mvectorstore, k\u001b[39m=\u001b[39mk, input_keys\u001b[39m=\u001b[39minput_keys)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_community/vectorstores/faiss.py:915\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m    897\u001b[0m \n\u001b[1;32m    898\u001b[0m \u001b[39mThis is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[39m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    914\u001b[0m embeddings \u001b[39m=\u001b[39m embedding\u001b[39m.\u001b[39membed_documents(texts)\n\u001b[0;32m--> 915\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m__from(\n\u001b[1;32m    916\u001b[0m     texts,\n\u001b[1;32m    917\u001b[0m     embeddings,\n\u001b[1;32m    918\u001b[0m     embedding,\n\u001b[1;32m    919\u001b[0m     metadatas\u001b[39m=\u001b[39;49mmetadatas,\n\u001b[1;32m    920\u001b[0m     ids\u001b[39m=\u001b[39;49mids,\n\u001b[1;32m    921\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    922\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_community/vectorstores/faiss.py:869\u001b[0m, in \u001b[0;36mFAISS.__from\u001b[0;34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    858\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__from\u001b[39m(\n\u001b[1;32m    859\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    868\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m FAISS:\n\u001b[0;32m--> 869\u001b[0m     faiss \u001b[39m=\u001b[39m dependable_faiss_import()\n\u001b[1;32m    870\u001b[0m     \u001b[39mif\u001b[39;00m distance_strategy \u001b[39m==\u001b[39m DistanceStrategy\u001b[39m.\u001b[39mMAX_INNER_PRODUCT:\n\u001b[1;32m    871\u001b[0m         index \u001b[39m=\u001b[39m faiss\u001b[39m.\u001b[39mIndexFlatIP(\u001b[39mlen\u001b[39m(embeddings[\u001b[39m0\u001b[39m]))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_community/vectorstores/faiss.py:58\u001b[0m, in \u001b[0;36mdependable_faiss_import\u001b[0;34m(no_avx2)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[39mimport\u001b[39;00m \u001b[39mfaiss\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     59\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import faiss python package. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install it with `pip install faiss-gpu` (for CUDA supported GPU) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `pip install faiss-cpu` (depending on Python version).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m     )\n\u001b[1;32m     63\u001b[0m \u001b[39mreturn\u001b[39;00m faiss\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version)."
     ]
    }
   ],
   "source": [
    "# SemanticSimilarityExampleSelector will select examples that are similiar to your input \n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "\n",
    "    examples, # this is a list os examples avaliable to select from.\n",
    "\n",
    "    OpenAIEmbeddings(openai_api_key=openai_api_key),\n",
    "\n",
    "    FAISS, # This is the vectorStore class we use that if we want to find the similar input . find the neigboors vectors\n",
    "\n",
    "    k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7c19a79b9aeb8b1cc18eda6778f62d726c8b19540b84d23ad80114035b2e0b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
